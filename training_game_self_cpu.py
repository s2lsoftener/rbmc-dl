#!/usr/bin/env python3

"""
File Name:      training_game_self_cpu.py
Authors:        Phong Tran
Date:           12/1/2020

Description:    Runs and saves self-play games using a loaded model. See the driver code
                at the bottom of this file.
"""

# %% Imports
import argparse
from torch import optim
from neural_net import AlphaZeroChess
from play_game import play_turn
import random
import chess
import chess.engine
from game import Game
from datetime import datetime
import time
from player import Player
from training_agent import TrainingAgent
import torch
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import copy
import joblib

# %% Methods for running the game
def play_local_game(white_player: TrainingAgent, black_player: TrainingAgent, game: Game):
    players = [black_player, white_player]

    white_player.handle_game_start(chess.WHITE, chess.Board())
    black_player.handle_game_start(chess.BLACK, chess.Board())
    game.start()

    # Need to track each player's state, the improved policy generated by MCTS, and the outcome
    states_policies = [[], []] # [black], [white]

    move_number = 1
    while not game.is_over():
        color = game.turn
        start = time.time()
        requested_move, taken_move, training_data = play_turn(game, players[game.turn], game.turn, move_number)
        print('TOTAL MOVE TIME:', time.time() - start)
        print()
        states_policies[color] += training_data
        move_number += 1

    winner_color, winner_reason = game.get_winner()
    white_player.handle_game_end(winner_color, winner_reason)
    black_player.handle_game_end(winner_color, winner_reason)

    return winner_color, winner_reason, states_policies

def play_turn(game: Game, player: TrainingAgent, turn: bool, move_number: int):
    # Set current color
    player_color = game.turn
    
    possible_moves = game.get_moves()
    possible_sense = list(chess.SQUARES)

    # Notify the player of the previous opponent's move
    captured_square = game.opponent_move_result()
    player.handle_opponent_move_result(captured_square is not None, captured_square)

    # Play sense action
    sense = player.choose_sense(possible_sense, possible_moves, game.get_seconds_left())
    sense_result = game.handle_sense(sense)
    print('Truth board')
    print(game.truth_board)
    player.handle_sense_result(sense_result)
    
    # Play move action
    move = player.choose_move(possible_moves, game.get_seconds_left())
    print('move passed to game handler', move)
    requested_move, taken_move, captured_square, reason = game.handle_move(move)
    player.handle_move_result(requested_move, taken_move, reason, captured_square is not None,
                              captured_square)

    # Retrieve the average board state the agent calculated when selecting its move.
    # This is the state that needs to be added to their [color]_states_policies in play_local_game
    # avg_board = player.board # type: np.ndarray

    # Retrieve the improved policy generated by MCTS.
    # This is the improved policy that needs to be added to [color]_states_policies in play_local_game.
    # Once the episode is done, it's used in the nnet's loss function when it's training itself.
    # pi = player.pi # type: np.ndarray
    training_data = player.training_data
    
    game.end_turn()
    return requested_move, taken_move, training_data

def episode(nnet: AlphaZeroChess, engine: chess.engine.EngineProtocol):
    data = [] # tuple of (states, policies, and outcome)
    
    player1 = TrainingAgent(nnet, engine)
    player2 = TrainingAgent(nnet, engine)
    game = Game(seconds_left=1200)
    # game.truth_board = 

    win_color, win_reason, game_data = play_local_game(player1, player2, game)

    # Add the winner and loser to the game data
    blacks_data, whites_data = game_data

    result = 1 if win_color == chess.WHITE else -1
    white_full_data = [(d[0], d[1], result) for d in whites_data]

    result = 1 if win_color == chess.BLACK else -1
    black_full_data = [(d[0], d[1], result) for d in blacks_data]

    data += white_full_data
    data += black_full_data

    N = len(data)
    np_states = np.zeros((N, 14, 8, 8), dtype=float)
    np_policies = np.zeros((N, 4096), dtype=float)
    np_values = np.zeros((N), dtype=float)

    for n in range(N):
        s, p, v = data[n]
        # print(s, p, v)
        np_states[n] = s.reshape((14, 8, 8))
        np_policies[n] = p.reshape(4096)
        np_values[n] = v

    all_data = (np_states, np_policies, np_values)

    print('The winner is ', ["black", "white"][win_color], 'by', win_reason)
    return win_color, all_data

def evaluate_new_net(white_net: AlphaZeroChess, black_net: AlphaZeroChess):
    white = TrainingAgent(white_net)
    black = TrainingAgent(black_net)
    game = Game(seconds_left=1200)

    win_color, win_reason, game_data = play_local_game(white, black, game)

    return win_color

def train(nnet, optimizer, dataloader: DataLoader):
    nnet.train()
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    for epoch in range(15):
        print('Epoch:', epoch, 'of 15.')
        running_loss = 0
        for state, policy, value in dataloader:
            optimizer.zero_grad()
            pi_pred, v_pred = nnet(state)
            pi_pred = pi_pred.view(-1, 4096)

            # Calculate loss
            v_error = (value.view(-1) - v_pred.view(-1)) ** 2
            # print(policy.shape)
            # print(pi_pred.shape)
            pi_error = torch.sum(policy * torch.log((pi_pred + 1e-6)), axis=1)
            loss = (v_error - pi_error).mean()
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            # print('Loss:', loss.item())
        print('Epoch loss:', running_loss)

    return nnet

def policy_iteration(checkpoint):
    # Initialize Neural Network
    nnet = AlphaZeroChess()
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    nnet.to(device)
    nnet.load_state_dict(checkpoint['model_state_dict'])
    nnet.eval()
    optimizer = torch.optim.Adam(nnet.parameters(), lr=0.003)
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    data = []
    for i in range(10): # Number of times to try training total
        print("\n\nIteration", i, "of overall policy iteration.")
        for _ in range(3):
            _, d = episode(nnet)
            data += d
        old_net = copy.deepcopy(nnet)
        nnet = train(nnet, optimizer, data)

        # Pit the new NN against the old one, and check for >50% winrate
        nnet.eval()
        old_net.eval()
        new_net_wins = 0
        for _ in range(5):
            print("Evaluating new network as white...")
            winner = evaluate_new_net(nnet, old_net)
            if winner == chess.WHITE:
                new_net_wins += 1
        for _ in range(5):
            print("Evaluating new network as black...")
            winner = evaluate_new_net(old_net, nnet)
            if winner == chess.BLACK:
                new_net_wins += 1
        
        win_rate = new_net_wins / 10
        if win_rate > 0.5:
            print("NEW NETWORK WAS BETTER!")
            continue
        else:
            print("Old network was better...")
            nnet = old_net # Go back to the old network

# %% Driver
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Allows you to play against a bot. Useful for testing and debugging.')
    parser.add_argument('start', help='Path to first bot source file.')
    parser.add_argument('end', help='Path to second bot source file.')
    args = parser.parse_args()
    
    # Initialize stockfish engine
    # engine = chess.engine.SimpleEngine.popen_uci('/usr/games/stockfish')
    device = torch.device("cpu")
    # checkpoint = torch.load('pht_model-80.pt', map_location=lambda device, loc: device)
    
    # # Initialize neural net
    # print('Initializing neural network...')
    # nnet = AlphaZeroChess()
    # device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # nnet.to(device)
    # nnet.eval()

    # Load neural net
    checkpoint = torch.load('k-az10-0.pt', map_location=lambda device, loc: device)
    nnet = AlphaZeroChess()
    nnet.to(device)
    nnet.load_state_dict(checkpoint['model_state_dict'])
    nnet.eval()

    # run an episode 
    for i in range(int(args.start), int(args.end)):
        print('Using stockfish+nnet on cpu, running episode...', i)
        engine = chess.engine.SimpleEngine.popen_uci('/usr/games/stockfish')
        win_color, data = episode(nnet, engine)
        joblib.dump(data, './training_data_k/iter_4/i4-az10-1-wsl-' + str(i) + '.joblib')
        print('Saved joblib file.')