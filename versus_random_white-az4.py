# %% Imports
import argparse
from torch import optim
from neural_net import AlphaZeroChess
from play_game import play_turn
import random
import chess
import chess.engine
from game import Game
from datetime import datetime
import time
from player import Player
# from stockfish_supervised_agent import MyAgent
# from stochastic_stockfish_mcts import TrainingAgent
from training_agent import TrainingAgent
# from solo_alphazero_mcts import TrainingAgent
from random_agent import Random
import torch
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import copy
import joblib

# %% Methods for running the game
def play_local_game(white_player: TrainingAgent, black_player: TrainingAgent, game: Game):
    players = [black_player, white_player]

    white_player.handle_game_start(chess.WHITE, chess.Board())
    black_player.handle_game_start(chess.BLACK, chess.Board())
    game.start()

    # Need to track each player's state, the improved policy generated by MCTS, and the outcome
    states_policies = [[], []] # [black], [white]

    move_number = 1
    while not game.is_over():
        color = game.turn
        start = time.time()
        requested_move, taken_move, training_data = play_turn(game, players[game.turn], game.turn, move_number)
        print('TOTAL MOVE TIME:', time.time() - start)
        print()
        states_policies[color] += training_data
        move_number += 1

    winner_color, winner_reason = game.get_winner()
    white_player.handle_game_end(winner_color, winner_reason)
    black_player.handle_game_end(winner_color, winner_reason)

    return winner_color, winner_reason, states_policies

def play_turn(game: Game, player: TrainingAgent, turn: bool, move_number: int):
    # Set current color
    player_color = game.turn
    
    possible_moves = game.get_moves()
    possible_sense = list(chess.SQUARES)

    # Notify the player of the previous opponent's move
    captured_square = game.opponent_move_result()
    player.handle_opponent_move_result(captured_square is not None, captured_square)

    # Play sense action
    sense = player.choose_sense(possible_sense, possible_moves, game.get_seconds_left())
    sense_result = game.handle_sense(sense)
    print('Truth board')
    print(game.truth_board)
    player.handle_sense_result(sense_result)
    
    # Play move action
    move = player.choose_move(possible_moves, game.get_seconds_left())
    print('move passed to game handler', move)
    requested_move, taken_move, captured_square, reason = game.handle_move(move)
    player.handle_move_result(requested_move, taken_move, reason, captured_square is not None,
                              captured_square)

    # Retrieve the average board state the agent calculated when selecting its move.
    # This is the state that needs to be added to their [color]_states_policies in play_local_game
    # avg_board = player.board # type: np.ndarray

    # Retrieve the improved policy generated by MCTS.
    # This is the improved policy that needs to be added to [color]_states_policies in play_local_game.
    # Once the episode is done, it's used in the nnet's loss function when it's training itself.
    # pi = player.pi # type: np.ndarray
    training_data = [None]
    if player_color == False: # black
        training_data = player.training_data
    
    game.end_turn()
    return requested_move, taken_move, training_data

def episode(nnet: AlphaZeroChess, engine: chess.engine.EngineProtocol):
    data = [] # tuple of (states, policies, and outcome)
    
    player1 = Random() # black
    player2 = TrainingAgent(nnet, engine) # white
    game = Game(seconds_left=1200)

    win_color, win_reason, game_data = play_local_game(player1, player2, game)

    # Add the winner and loser to the game data
    blacks_data, whites_data = game_data

    # result = 1 if win_color == chess.WHITE else -1
    # white_full_data = [(d[0], d[1], result) for d in whites_data]

    result = 1 if win_color == chess.BLACK else -1
    black_full_data = [(d[0], d[1], result) for d in blacks_data]

    # data += white_full_data
    data += black_full_data

    print('The winner is ', ["black", "white"][win_color], 'by', win_reason)
    return win_color, data, win_reason


# %% Driver
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Allows you to play against a bot. Useful for testing and debugging.')
    parser.add_argument('start', help='Path to first bot source file.')
    parser.add_argument('end', help='Path to second bot source file.')
    args = parser.parse_args()
    
    # Initialize stockfish engine
    # engine = chess.engine.SimpleEngine.popen_uci('/usr/games/stockfish')
    device = torch.device("cpu")
    
    # # Initialize neural net
    # print('Initializing neural network...')
    # nnet = AlphaZeroChess()
    # device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    # nnet.to(device)
    # nnet.eval()

    # Load neural net
    checkpoint = torch.load('k-az4.pt', map_location=lambda device, loc: device)
    nnet = AlphaZeroChess()
    nnet.to(device)
    nnet.load_state_dict(checkpoint['model_state_dict'])
    nnet.eval()

    # run an episode
    winners = []
    for i in range(int(args.start), int(args.end)):
        print('Using stockfish+nnet on cpu, running episode...', i)
        engine = chess.engine.SimpleEngine.popen_uci('/usr/games/stockfish')
        win_color, data, win_reason = episode(nnet, engine)
        winners.append(win_color)
        win_percent = (np.array(winners) == False).sum() / len(winners)
        print("!!!!AlphaZero has won percentage of games:", str(win_percent))
        joblib.dump(data, './output/k-az4-bVSw-random-' + str(i) + '.joblib')
        outputxt = open("az-4_vs_randwhite.txt", "a")
        outputxt.write(["BLACK", "WHITE"][win_color] + ": " + win_reason + "\n")
        outputxt.close()
        print('Saved joblib file.\n\n')